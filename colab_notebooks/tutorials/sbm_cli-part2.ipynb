{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["-GhrvkOFT9_t"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}}},"cells":[{"cell_type":"markdown","source":["# Tutorial no. 2 of SpeechBrain-MOABB: Setting up hyper-parameter tuning"],"metadata":{"id":"FLmjnvhyullP"}},{"cell_type":"markdown","metadata":{"id":"bkw1jVAnpBQ9"},"source":["## **Prerequisites**\n"]},{"cell_type":"markdown","source":["### Download SpeechBrain-MOABB\n","\n","SpeechBrain-MOABB can be downloaded from the GitHub repository listed below."],"metadata":{"id":"-GhrvkOFT9_t"}},{"cell_type":"code","metadata":{"id":"KSOTkqPsJXxZ"},"source":["%%capture\n","!git clone https://github.com/speechbrain/benchmarks\n","%cd /content/benchmarks\n","!git checkout eeg\n","\n","%cd /content/benchmarks/benchmarks/MOABB\n","!pip install -r extra-requirements.txt # Install additional dependencies"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vxr8nZ7bbP15"},"source":["### Install SpeechBrain and SpeechBrain-MOABB requirements, and install SpeechBrain"]},{"cell_type":"code","metadata":{"id":"VFEX28mu4dFt"},"source":["%%capture\n","# Clone SpeechBrain repository (development branch)\n","%cd /content/\n","!git clone https://github.com/speechbrain/speechbrain/\n","%cd /content/speechbrain/\n","\n","# Install required dependencies\n","!pip install -r requirements.txt\n","\n","# Install SpeechBrain in editable mode\n","!pip install -e .\n","\n","%cd /content/\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Define the yaml file including the hyper-parameter search space**\n","\n","Let us use the same yaml file as in the *Tutorial no. 1 of SpeechBrain-MOABB: Setting up EEG decoding*. However, in this case we assume that some hyper-parameters are not optimal. For example, we assume that the low and high cut-off frequencies for band-pass filtering should be optimized, together with the number of epochs, the learning rate, and few network hyper-parameters (e.g., the number of convolutional kernels and the kernel size of the first layer of EEGNet).\n","\n","We provide a CLI for performing hyper-parameter search, by using the `./run_hparam_optimization.sh` script, that performs the hyper-parameter search iterations calling multiple times the `./run_experiments.sh` script. The script assumes that Orion flags are directly included in the specified YAML hyper-parameter file using comments. Thus, you can easily define the search space for each hyper-parameter by commenting:\n","\n","```yaml\n","dropout: 0.1748  # @orion_step1: --dropout~\"uniform(0.0, 0.5)\"\n","```\n","\n","In this case, dropout rate will be sampled using an uniform distribution between 0 and 0.5. See Orion documentation for the supported distributions.\n","\n","`./run_hparam_optimization.sh` supports multi-step hyper-parameter optimization.\n","Briefly, you can optimize a subset of hyper-parameters while keeping the others fixed. After finding their optimal values, we utilize them as a foundation for optimizing another set of hyper-parameters. Furthermore, once hyper-parameter tuning is completed, the optimal decoding pipeline is re-trained and re-evaluated for N times for providing a robust evaluation of the performance, to reduce the variability of the performance due to different random initializations (seed variability), by setting the parameter `nruns_eval`. Besides the options provided by the `./run_experiments.sh` CLI, `./run_hparam_optimization.sh` introduces other options. For example, the user can change the amount of signals to use during hyper-parameter search, crucial for reducing computational time on large datasets, by setting the number of participants and sessions to use for hyper-parameter search (`nsbj_hpsearch`, `nsess_hpsearch`). The number of iterations performed during hyper-parameter search is identified by `exp_max_trials`.\n","\n","To optimize a hyper-parameter in a second step, follow this syntax in the YAML file:\n","\n","```yaml\n","# cutcat (disabled when min_num_segments=max_num_segments=1)\n","max_num_segments: 6 # @orion_step2: --max_num_segments~\"uniform(2, 6, discrete=True)\"\n","```\n","\n","For brevity, the number of epochs was tuned here only between 50 and 200 epochs.\n","Moreover, for brevity we set the following CLI options (for demonstration purposes only):\n","```\n","--nsbj_hpsearch 1 --nsess_hpsearch 1 \\\n","--nruns_eval 1 \\\n","--exp_max_trials 5\n","```\n","Of course, users are encouraged to increase these values for obtaining higher decoding performance (see the recommended values in the repository and in the associated SpeechBrain-MOABB paper).\n"],"metadata":{"id":"f45uzf3mUNyc"}},{"cell_type":"code","source":["tuned_hyperparams = \"\"\"\n","# DATASET HPARS\n","fmin: 1 # @orion_step1: --fmin~\"uniform(0.1, 5, precision=2)\"\n","fmax: 40 # @orion_step1: --fmax~\"uniform(20.0, 50.0, precision=3)\"\n","\n","# TRAINING HPARS\n","number_of_epochs: 1000 # @orion_step1: --number_of_epochs~\"uniform(50, 200, discrete=True)\"\n","lr: 0.0001 # @orion_step1: --lr~\"choices([0.01, 0.005, 0.001, 0.0005, 0.0001])\"\n","\n","# MODEL\n","cnn_temporal_kernels: 8 # @orion_step1: --cnn_temporal_kernels~\"uniform(4, 64,discrete=True)\"\n","cnn_temporal_kernelsize: 62 # @orion_step1: --cnn_temporal_kernelsize~\"uniform(24, 62,discrete=True)\"\n","\n","\"\"\"\n","\n","other_hyperparams = \"\"\"\n","seed: 1234\n","__set_torchseed: !apply:torch.manual_seed [!ref <seed>]\n","\n","# DIRECTORIES\n","data_folder: !PLACEHOLDER  #'/path/to/dataset'. The dataset will be automatically downloaded in this folder\n","cached_data_folder: !PLACEHOLDER #'path/to/pickled/dataset'\n","output_folder: !PLACEHOLDER #'path/to/results'\n","\n","# DATASET HPARS\n","# Defining the MOABB dataset.\n","dataset: !new:moabb.datasets.BNCI2014001\n","save_prepared_dataset: True # set to True if you want to save the prepared dataset as a pkl file to load and use afterwards\n","data_iterator_name: 'leave-one-session-out'\n","target_subject_idx: 0\n","target_session_idx: 1\n","events_to_load: null # all events will be loaded\n","original_sample_rate: 250 # Original sampling rate provided by dataset authors\n","sample_rate: 125 # Target sampling rate (Hz)\n","# band-pass filtering cut-off frequencies\n","n_classes: 4\n","tmin: 0.\n","tmax: 4.0\n","# number of steps used when selecting adjacent channels from a seed channel (default at Cz)\n","n_steps_channel_selection: 3\n","T: !apply:math.ceil\n","    - !ref <sample_rate> * (<tmax> - <tmin>)\n","C: 22\n","test_with: 'last' # 'last' or 'best'\n","test_key: \"acc\" # Possible opts: \"loss\", \"f1\", \"auc\", \"acc\"\n","\n","# METRICS\n","f1: !name:sklearn.metrics.f1_score\n","    average: 'macro'\n","acc: !name:sklearn.metrics.balanced_accuracy_score\n","cm: !name:sklearn.metrics.confusion_matrix\n","metrics:\n","    f1: !ref <f1>\n","    acc: !ref <acc>\n","    cm: !ref <cm>\n","\n","# TRAINING HPARS\n","n_train_examples: 100  # it will be replaced in the train script\n","# checkpoints to average\n","avg_models: 10\n","# Learning rate scheduling (cyclic learning rate is used here)\n","max_lr: !ref <lr> # Upper bound of the cycle (max value of the lr)\n","base_lr: 0.00000001 # Lower bound in the cycle (min value of the lr)\n","step_size_multiplier: 5 #from 2 to 8\n","step_size: !apply:round\n","    - !ref <step_size_multiplier> * <n_train_examples> / <batch_size>\n","lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler\n","    base_lr: !ref <base_lr>\n","    max_lr: !ref <max_lr>\n","    step_size: !ref <step_size>\n","label_smoothing: 0.0\n","loss: !name:speechbrain.nnet.losses.nll_loss\n","    label_smoothing: !ref <label_smoothing>\n","optimizer: !name:torch.optim.Adam\n","    lr: !ref <lr>\n","epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter  # epoch counter\n","    limit: !ref <number_of_epochs>\n","batch_size: 32\n","valid_ratio: 0.2\n","\n","# DATA NORMALIZATION\n","dims_to_normalize: 1 # 1 (time) or 2 (EEG channels)\n","normalize: !name:speechbrain.processing.signal_processing.mean_std_norm\n","    dims: !ref <dims_to_normalize>\n","\n","# MODEL\n","input_shape: [null, !ref <T>, !ref <C>, null]\n","cnn_spatial_depth_multiplier: 2\n","cnn_spatial_max_norm: 1.\n","cnn_spatial_pool: 4\n","cnn_septemporal_depth_multiplier: 1\n","cnn_septemporal_point_kernels: !ref <cnn_temporal_kernels> * <cnn_spatial_depth_multiplier> * <cnn_septemporal_depth_multiplier>\n","cnn_septemporal_kernelsize: 16\n","cnn_septemporal_pool: 8\n","cnn_pool_type: 'avg'\n","dense_max_norm: 0.25\n","dropout: 0.5\n","activation_type: 'elu'\n","\n","model: !new:models.EEGNet.EEGNet\n","    input_shape: !ref <input_shape>\n","    cnn_temporal_kernels: !ref <cnn_temporal_kernels>\n","    cnn_temporal_kernelsize: [!ref <cnn_temporal_kernelsize>, 1]\n","    cnn_spatial_depth_multiplier: !ref <cnn_spatial_depth_multiplier>\n","    cnn_spatial_max_norm: !ref <cnn_spatial_max_norm>\n","    cnn_spatial_pool: [!ref <cnn_spatial_pool>, 1]\n","    cnn_septemporal_depth_multiplier: !ref <cnn_septemporal_depth_multiplier>\n","    cnn_septemporal_point_kernels: !ref <cnn_septemporal_point_kernels>\n","    cnn_septemporal_kernelsize: [!ref <cnn_septemporal_kernelsize>, 1]\n","    cnn_septemporal_pool: [!ref <cnn_septemporal_pool>, 1]\n","    cnn_pool_type: !ref <cnn_pool_type>\n","    activation_type: !ref <activation_type>\n","    dense_max_norm: !ref <dense_max_norm>\n","    dropout: !ref <dropout>\n","    dense_n_neurons: !ref <n_classes>\n","\n","\"\"\"\n","\n","sample_hyperparams = tuned_hyperparams + other_hyperparams"],"metadata":{"id":"f_m7D3eiUbHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the yaml file on disk\n","f = open('/content/sample_hyperparams.yaml', \"w\")\n","f.write(sample_hyperparams)\n","f.close()"],"metadata":{"id":"DbJcCq8fYgL-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/benchmarks/benchmarks/MOABB/\n","\n","!./run_hparam_optimization.sh --hparams '/content/sample_hyperparams.yaml' \\\n","--data_folder '/content/data/BNCI2014001'\\\n","--cached_data_folder '/content/data' \\\n","--output_folder '/content/results/hyperparameter-search/BNCI2014001' \\\n","--nsbj 9 --nsess 2 --nruns 1 --train_mode 'leave-one-session-out' \\\n","--exp_name 'hyperparameter-search' \\\n","--nsbj_hpsearch 1 --nsess_hpsearch 1 \\\n","--nruns_eval 1 \\\n","--eval_metric acc \\\n","--exp_max_trials 5\n","\n"],"metadata":{"id":"VJaBar0Iuw3H"},"execution_count":null,"outputs":[]}]}