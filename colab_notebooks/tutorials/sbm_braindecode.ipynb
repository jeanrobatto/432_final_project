{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["kd6sYnYM1Hd6"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"pycharm":{"stem_cell":{"cell_type":"raw","source":[],"metadata":{"collapsed":false}}}},"cells":[{"cell_type":"markdown","source":["# Tutorial no. 3 of SpeechBrain-MOABB: Integrating braindecode models"],"metadata":{"id":"vSB-QzlYVCYY"}},{"cell_type":"markdown","metadata":{"id":"JnRSS3jGxVsk"},"source":["## **Prerequisites**\n"]},{"cell_type":"markdown","source":["### Download SpeechBrain-MOABB\n","\n","SpeechBrain-MOABB can be downloaded from the GitHub repository listed below."],"metadata":{"id":"31QRi2ZqxZfB"}},{"cell_type":"code","metadata":{"id":"swPvPkNrxZfC"},"source":["%%capture\n","!git clone https://github.com/speechbrain/benchmarks\n","%cd /content/benchmarks\n","!git checkout eeg\n","\n","%cd /content/benchmarks/benchmarks/MOABB\n","!pip install -r extra-requirements.txt # Install additional dependencies"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xVjvUfUJxZfC"},"source":["### Install SpeechBrain and SpeechBrain-MOABB requirements, and install SpeechBrain"]},{"cell_type":"code","metadata":{"id":"sfbgIBQPxZfD"},"source":["%%capture\n","# Clone SpeechBrain repository (development branch)\n","%cd /content/\n","!git clone https://github.com/speechbrain/speechbrain/\n","%cd /content/speechbrain/\n","\n","# Install required dependencies\n","!pip install -r requirements.txt\n","\n","# Install SpeechBrain in editable mode\n","!pip install -e .\n","\n","%cd /content/benchmarks/benchmarks/MOABB\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kd6sYnYM1Hd6"},"source":["### Install braindecode"]},{"cell_type":"code","metadata":{"id":"BkjIu0hD1Hd9"},"source":["%%capture\n","!pip install braindecode"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Define the yaml file integrating a braindecode model**\n","\n","Based on the yaml file defined in the tutorial no. 1, let us assume that we are interested into using a model already developed in braindecode library. SpeechBrain-MOABB includes the model class `BraindecodeNN` that transforms a braindecode model for being compliant with SpeechBrain and SpeechBrain-MOABB. Then, the braindecode model can be integrated seamlessly with the usual decoding pipeline of SpeechBrain-MOABB. For example, for using EEGInception model (see https://braindecode.org/stable/generated/braindecode.models.EEGInception.html), we can do the following."],"metadata":{"id":"f45uzf3mUNyc"}},{"cell_type":"code","source":["model_hyperparams = \"\"\"\n","# MODEL\n","input_shape: [null, !ref <T>, !ref <C>, null]\n","braindecode_model: !new:braindecode.models.EEGInception\n","    n_outputs: !ref <n_classes>\n","    n_chans: !ref <C>\n","    n_times: !ref <T>\n","    add_log_softmax: True\n","\n","model: !new:models.BraindecodeNN.BraindecodeNN\n","    model: !ref <braindecode_model>\n","\n","\"\"\""],"metadata":{"id":"IYzVM2gp3fM1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, we can add this model hyper-parameters to the other hyper-parameters defining the decoding pipeline."],"metadata":{"id":"rpoTepu-3oJ7"}},{"cell_type":"code","source":["example_hyperparams = \"\"\"\n","seed: 1234\n","__set_torchseed: !apply:torch.manual_seed [!ref <seed>]\n","\n","# DIRECTORIES\n","data_folder: !PLACEHOLDER  #'/path/to/dataset'. The dataset will be automatically downloaded in this folder\n","cached_data_folder: !PLACEHOLDER #'path/to/pickled/dataset'\n","output_folder: !PLACEHOLDER #'path/to/results'\n","\n","# DATASET HPARS\n","# Defining the MOABB dataset.\n","dataset: !new:moabb.datasets.BNCI2014001\n","save_prepared_dataset: True # set to True if you want to save the prepared dataset as a pkl file to load and use afterwards\n","data_iterator_name: 'leave-one-session-out'\n","target_subject_idx: 0\n","target_session_idx: 1\n","events_to_load: null # all events will be loaded\n","original_sample_rate: 250 # Original sampling rate provided by dataset authors\n","sample_rate: 125 # Target sampling rate (Hz)\n","# band-pass filtering cut-off frequencies\n","fmin: 1\n","fmax: 40\n","n_classes: 4\n","tmin: 0.\n","tmax: 4.0\n","# number of steps used when selecting adjacent channels from a seed channel (default at Cz)\n","n_steps_channel_selection: 3\n","T: !apply:math.ceil\n","    - !ref <sample_rate> * (<tmax> - <tmin>)\n","C: 22\n","test_with: 'best' # 'last' or 'best'\n","test_key: \"acc\" # Possible opts: \"loss\", \"f1\", \"auc\", \"acc\"\n","\n","# METRICS\n","f1: !name:sklearn.metrics.f1_score\n","    average: 'macro'\n","acc: !name:sklearn.metrics.balanced_accuracy_score\n","cm: !name:sklearn.metrics.confusion_matrix\n","metrics:\n","    f1: !ref <f1>\n","    acc: !ref <acc>\n","    cm: !ref <cm>\n","\n","# TRAINING HPARS\n","n_train_examples: 100  # it will be replaced in the train script\n","# checkpoints to average\n","avg_models: 1\n","number_of_epochs: 1000\n","lr: 0.001\n","# Learning rate scheduling (cyclic learning rate is used here)\n","max_lr: !ref <lr> # Upper bound of the cycle (max value of the lr)\n","base_lr: 0.00000001 # Lower bound in the cycle (min value of the lr)\n","step_size_multiplier: 5 #from 2 to 8\n","step_size: !apply:round\n","    - !ref <step_size_multiplier> * <n_train_examples> / <batch_size>\n","lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler\n","    base_lr: !ref <base_lr>\n","    max_lr: !ref <max_lr>\n","    step_size: !ref <step_size>\n","label_smoothing: 0.0\n","loss: !name:speechbrain.nnet.losses.nll_loss\n","    label_smoothing: !ref <label_smoothing>\n","optimizer: !name:torch.optim.Adam\n","    lr: !ref <lr>\n","epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter  # epoch counter\n","    limit: !ref <number_of_epochs>\n","batch_size: 32\n","valid_ratio: 0.2\n","\n","# DATA NORMALIZATION\n","dims_to_normalize: 1 # 1 (time) or 2 (EEG channels)\n","normalize: !name:speechbrain.processing.signal_processing.mean_std_norm\n","    dims: !ref <dims_to_normalize>\n","\n","\"\"\"\n","\n","example_hyperparams += model_hyperparams"],"metadata":{"id":"f_m7D3eiUbHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the yaml file on disk\n","f = open('/content/example_hyperparams.yaml', \"w\")\n","f.write(example_hyperparams)\n","f.close()"],"metadata":{"id":"DbJcCq8fYgL-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Train the neural network on a single cross-validation fold**\n","\n","Finally, we can train the network as done in tutorial no. 1. For brevity, we override also here the number of training epochs to 50 epochs. Increase the number of epochs for obtaining higher accuracies.\n"],"metadata":{"id":"qiOA1AlVg48b"}},{"cell_type":"code","source":["!python train.py /content/example_hyperparams.yaml \\\n","--data_folder '/content/data/BNCI2014001' \\\n","--cached_data_folder '/content/data' \\\n","--output_folder '/content/results/single-fold-example-braindecode/BNCI2014001' \\\n","--data_iterator_name 'leave-one-session-out' \\\n","--target_subject_idx 0 \\\n","--target_session_idx 1 \\\n","--number_of_epochs 50 \\\n","--device 'cpu'"],"metadata":{"id":"pYEa8oubbvk-","pycharm":{"name":"#%%\n"}},"execution_count":null,"outputs":[]}]}